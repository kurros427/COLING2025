{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load necessaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers==4.44.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    raise Exception('No GPU available')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = {\n",
    "'basic' :\"\"\"请仔细阅读以下多条文本,然后回答问题：基于这些文本，这个用户是否存在抑郁情绪？用“是”或者“否”回答\n",
    "***重要***\n",
    "你必须用以下格式回答问题\n",
    "答案：“是”或“否”\n",
    "\"\"\",\n",
    "'OR': \"\"\"请仔细阅读以下多条文本，关注用户的情绪表达、对未来的态度、社交参与度、兴趣水平以及任何关于身体症状或睡眠/食欲变化、自我价值、个人感受的表述。回答问题：基于这些文本，这个用户是否存在抑郁情绪？用“是”或者“否”回答，再给出你一步一步的分析过程。\n",
    "***重要***\n",
    "你必须用以下格式回答问题\n",
    "答案：“是”或“否”\n",
    "分析：你一步一步的分析过程，解释你为什么给出这个答案。\"\"\",\n",
    "'OR2': \"\"\"请仔细阅读以下多条文本然后回答问题：基于这些文本，这个用户是否存在抑郁情绪？用“是”或者“否”回答，再给出你一步一步的分析过程。\n",
    "***重要***\n",
    "你必须用以下格式回答问题\n",
    "答案：“是”或“否”\n",
    "分析：你一步一步的分析过程，解释你为什么给出这个答案。\"\"\",\n",
    "'OR3': \"\"\"请仔细阅读以下多条文本然后回答问题：基于这些文本，这个用户是否存在抑郁情绪？用“是”或者“否”回答，再给出你一步一步的分析过程。\n",
    "\"\"\",\n",
    "\n",
    "'RET':\"\"\"请逐条阅读以下用户的文本，关注用户的情绪表达、对未来的态度、社交参与度、兴趣水平以及任何关于身体症状或睡眠/食欲变化、自我价值、个人感受的表述。回答问题：基于这些文本，这个用户是否存在抑郁情绪？用“是”或者“否”回答，再给出你一步一步的分析过程，指出哪条或哪几条文本让你得出这个回答。\n",
    "***重要***\n",
    "如果用户在文本中明确提到自己得到了抑郁的诊断，请直接判断为“是”，并在分析中指出相关文本。\n",
    "你必须用以下格式回答问题\n",
    "答案：“是”或“否”\n",
    "分析：你一步一步的分析过程，指出哪条或哪几条文本让你得出这个回答。\"\"\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_tweet (tweet,mode = 1):\n",
    "    input_text_list = tweet.split('\\n')\n",
    "    if mode == 0 :\n",
    "        tweet = tweet\n",
    "    elif mode == 1:\n",
    "        input_text_list_mod= [f'文本{i+1}:{text}' for i,text in enumerate(input_text_list)]\n",
    "        tweet = '\\n'.join(input_text_list_mod)\n",
    "    elif mode == 2:\n",
    "        input_text_list_mod = [f'<文本{i+1}>{text}</文本{i+1}> ' for i,text in enumerate(input_text_list)]\n",
    "        tweet = '\\n'.join(input_text_list_mod)\n",
    "    return tweet\n",
    "# RMSNorm normalization\n",
    "def rmsnorm(x, eps=1e-6): #x: Input tensor，eps: Small constant to prevent division by zero，return: Normalized tensor\n",
    "    rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + eps)\n",
    "    return x / rms\n",
    "\n",
    "def find_start_end_index(start_token = '<|im_start|>',end_token ='<|im_end|>',drift = 3,glm = False):\n",
    "    input_start = 0\n",
    "    input_end = 0\n",
    "    output_start = 0\n",
    "    output_end = 0\n",
    "    input_count = 0\n",
    "    output_count = 0\n",
    "    i = 0\n",
    "    for token in generated_ids[0][0]:\n",
    "        if glm :\n",
    "            token = [token]\n",
    "        if tokenizer.decode(token) == start_token:\n",
    "            if input_count == 0:\n",
    "                input_start = i+drift\n",
    "                input_count += 1\n",
    "            else:\n",
    "                output_start = i+drift\n",
    "        elif tokenizer.decode(token) == end_token:\n",
    "            if output_count == 0:\n",
    "                input_end = i\n",
    "                output_count += 1\n",
    "            else:\n",
    "                output_end = i\n",
    "        i += 1\n",
    "    print('input_start:',input_start,'input_end:',input_end)\n",
    "    print('output_start:',output_start,'output_end:',output_end)\n",
    "    assert input_start < input_end\n",
    "    #assert output_start < output_end\n",
    "    return input_start,input_end,output_start,output_end\n",
    "\n",
    "def compute_attention_rollout(converted_attentions,response,norm_type = 'RMS',add_residual=True, eps=1e-6):\n",
    "    all_attentions =  [(t,a) for t,a in zip(response.tolist(), converted_attentions)]\n",
    "    attention_scores =[]\n",
    "    for pair in all_attentions:\n",
    "        token = pair[0]\n",
    "        att_mat = pair[1].mean(axis=1) #Calculate the average attention head weights to obtain a tensor of shape [layer, seq_len or 1, seq_lenor or seq_lenor + generation steps]\n",
    "\n",
    "        if add_residual:\n",
    "            att_mat = att_mat + np.eye(att_mat.shape[1])[None,...] #residual connection\n",
    "        if norm_type == 'RMS':\n",
    "            att_mat = rmsnorm(torch.tensor(att_mat), eps=eps).numpy() #RMSNorm normalization\n",
    "        elif norm_type == 'layernorm':\n",
    "            att_mat = att_mat / att_mat.sum(axis=-1)[...,None] #Layer normalization\n",
    "        att_mat = rmsnorm(torch.tensor(att_mat), eps=eps).numpy()\n",
    "        joint_att = np.zeros(att_mat.shape)\n",
    "        layers = joint_att.shape[0]\n",
    "        joint_att[0] = att_mat[0]\n",
    "        for i in np.arange(1, layers):\n",
    "            joint_att[i] = att_mat[i].dot(joint_att[i-1].T)\n",
    "        attention_scores.append((token,joint_att))\n",
    "    return attention_scores\n",
    "# Generate an HTML file based on the input vector and text, where each character displays its corresponding score above it.\n",
    "# Adjust the color based on the score. Save the final HTML\n",
    "# Parameters:\n",
    "# tensor (numpy.ndarray): A vector of shape (1, N), where N is the length of the text.\n",
    "# text (str): A text string matching the length of the vector.\n",
    "# output_path (str): The path to save the output HTML file.\n",
    "# normalize (bool): Whether to normalize the scores, default is True.\n",
    "# method (str): Normalization method, supports 'min-max' (default), 'mean', and 'moving-average'.\n",
    "# window_size (int): Window size for moving average, default is 10, used only when moving average normalization is selected.\n",
    "\n",
    "# Returns:\n",
    "# None\n",
    "def generate_text_with_scores_html(tensor, text, output_path, normalize=True, method='min-max', window_size=10):\n",
    "    \n",
    "    # Check the shape of the tensor and the length of the text\n",
    "    assert tensor.shape[1] == len(text), \"Tensor length must match the text length.\"\n",
    "\n",
    "    scores = tensor[0]\n",
    "\n",
    "    # Normalize the scores\n",
    "    if normalize:\n",
    "        if method == 'min-max':\n",
    "            scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "        elif method == 'mean':\n",
    "            mean_value = scores.mean()\n",
    "            scores = scores - mean_value\n",
    "        elif method == 'moving-average':\n",
    "            mean_value = np.convolve(scores, np.ones(window_size) / window_size, mode='same')\n",
    "            scores = scores - mean_value\n",
    "        elif method == 'z-score':\n",
    "            mean_value = scores.mean()\n",
    "            std_value = scores.std()\n",
    "            scores = (scores - mean_value) / std_value\n",
    "        else:\n",
    "            raise ValueError(\"Unknown normalization method! Please choose from 'min-max', 'mean', 'moving-average', or 'z-score'.\")\n",
    "\n",
    "    # Convert the score to color\n",
    "    def score_to_color(score):\n",
    "        r = int(255 * score)\n",
    "        b = 255 - r\n",
    "        return f'rgb({r}, 0, {b})'\n",
    "\n",
    "    # Calculate the number of lines\n",
    "    chars_per_line = 50\n",
    "    num_lines = len(text) // chars_per_line + (1 if len(text) % chars_per_line else 0)\n",
    "\n",
    "    # Generate the HTML content\n",
    "    html_content = \"<html><body style='font-family:monospace;'>\\n\"\n",
    "\n",
    "    # Add the text with scores\n",
    "    spacing = \"20px\"  # Adjust the spacing between characters\n",
    "\n",
    "    for line in range(num_lines):\n",
    "        start_idx = line * chars_per_line\n",
    "        end_idx = start_idx + chars_per_line\n",
    "        line_text = text[start_idx:end_idx]\n",
    "        line_scores = scores[start_idx:end_idx]\n",
    "\n",
    "        for i, char in enumerate(line_text):\n",
    "            color = score_to_color(line_scores[i])\n",
    "            score_text = f\"{line_scores[i]:.2f}\"\n",
    "            border_style = \"border: 1px solid black;\" if line_scores[i] > 0 else \"\"\n",
    "            html_content += f\"<div style='display:inline-block; text-align:center; color:{color}; margin-right:{spacing}; {border_style}'>\" \\\n",
    "                            f\"<div style='font-size:0.5em;'>{score_text}</div>\" \\\n",
    "                            f\"<div>{char}</div>\" \\\n",
    "                            f\"</div>\"\n",
    "\n",
    "        html_content += \"<br>\\n\"\n",
    "\n",
    "    html_content += \"</body></html>\"\n",
    "\n",
    "    # Save the HTML content to a file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "    print(f\"HTML Generated at {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open ('datas_positive.json','r',encoding='utf-8') as f:\n",
    "  datas_positive = json.load(f)\n",
    "with open ('datas_negative.json','r',encoding='utf-8') as f:\n",
    "  datas_negative = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sample = datas_positive['user_42']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"shenzhi-wang/Llama3-8B-Chinese-Chat\"\n",
    "#model_id = \"D:\\LLMs\\Llama3-8B-Chinese-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype= torch.bfloat16, device_map=\"auto\",attn_implementation=\"eager\", \n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = system_prompt['OR2']+'\\n'+modify_tweet(tweet_sample,mode=0)\n",
    "prompt_length = len(tokenizer(system_prompt['OR2'])['input_ids'])\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"system\", \"content\": prompt}],\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"system\", \"content\": prompt}],\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "generate_params_llama = {\n",
    "                    # Change this based on how much vram you have,in our experiments, we set it to 350\n",
    "                    # so the final attention rollout may vary if you set it to a different value\n",
    "                     'max_new_tokens': 100, \n",
    "                     'pad_token_id': tokenizer.pad_token_id,\n",
    "                     #'terminator_ids': terminators,\n",
    "                     'do_sample': False,\n",
    "                     'output_attentions': True,\n",
    "                     'return_dict_in_generate': True,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        **generate_params_llama\n",
    "        )\n",
    "response_ids = generated_ids[0][0][input_ids.shape[-1]:]\n",
    "response = tokenizer.decode(response_ids, skip_special_tokens=False)\n",
    "print(response)\n",
    "start_index,end_index,_,_ = find_start_end_index(start_token = '<|end_header_id|>',end_token ='<|eot_id|>',drift = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QWEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2-7B-Instruct\"\n",
    "#model_id = \"D:\\LLMs\\Qwen2-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_generate_params = {\n",
    "    # Change this based on how much vram you have,in our experiments, we set it to 350\n",
    "    # so the final attention rollout may vary if you set it to a different value\n",
    "    \"max_new_tokens\": 100, \n",
    "    \"output_attentions\": True,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "prompt = system_prompt['OR2']+'\\n'+modify_tweet(tweet_sample,mode=0)\n",
    "prompt_length = len(tokenizer(system_prompt['OR2'])['input_ids'])\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\":prompt },\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    generated_ids = model.generate(model_inputs.input_ids, **qwen_generate_params)\n",
    "response_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids[0])]\n",
    "response = tokenizer.batch_decode(response_ids, skip_special_tokens=False)[0]\n",
    "print(response)\n",
    "start_index,end_index,_,_ = find_start_end_index(drift=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = \"THUDM/glm-4-9b-chat\"\n",
    "model_id = \"D:\\LLMs\\glm-4-9b-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\" ,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the generation code so that it returns attentions matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a must for glm model,sinice GLM use its own custom generation codes (modeling_chatglm.py)\n",
    "#Which dose not output attentions by default\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub/\")\n",
    "target_file = \"modeling_chatglm.py\"\n",
    "\n",
    "\n",
    "def find_file(directory, filename):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if filename in files:\n",
    "            return os.path.join(root, filename)\n",
    "    return None\n",
    "\n",
    "file_path = find_file(cache_dir, target_file)\n",
    "\n",
    "if file_path:\n",
    "    print(f\"Found: {file_path}\")\n",
    "    model_path = os.path.dirname(file_path)\n",
    "    backup_file = os.path.join(model_path, \"modeling_chatglm.py.bak\")\n",
    "    os.rename(file_path, backup_file)\n",
    "    print(f\"Renamed original file to: {backup_file}\")\n",
    "    script_file = \"modeling_chatglm.py\"\n",
    "    destination = os.path.join(model_path, script_file)\n",
    "    shutil.copy(script_file, destination)\n",
    "    print(f\"Copied new script to: {destination}\")   \n",
    "else:\n",
    "    print(\"File not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = system_prompt['OR2']+'\\n'+modify_tweet(tweet_sample,mode=0)\n",
    "prompt_length = len(tokenizer(system_prompt['OR2'])['input_ids'])\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"system\", \"content\": prompt}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       ).to(model.device)\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    # Change this based on how much vram you have,in our experiments, we set it to 350\n",
    "    # so the final attention rollout may vary if you set it to a different value\n",
    "    max_new_tokens=100,\n",
    "    do_sample = False,\n",
    "    output_attentions=True,\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "response_ids = generated_ids[0][:, inputs['input_ids'].shape[1]:]\n",
    "print(tokenizer.decode(response_ids[0], skip_special_tokens=False))\n",
    "start_index,end_index,_,_ = find_start_end_index(start_token='<|system|>',end_token='<|assistant|>',drift=2,glm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout Calculations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "beta = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention Rollout (Input seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = start_index\n",
    "t = end_index\n",
    "attention_input = [\n",
    "    torch.max(hd[:, :, s:t, s:t].cpu().squeeze(0).to(torch.float32), dim=0)[0]\n",
    "    for hd in generated_ids['attentions'][0]\n",
    "]\n",
    "print(len(attention_input))\n",
    "print(attention_input[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviced_attention = []\n",
    "attender_attention = []\n",
    "recived_pairs = []\n",
    "index_tokens = [ tokenizer.decode(t,skip_special_tokens=False) for t in generated_ids[0][0][s:t]]\n",
    "seq_len = len(index_tokens)\n",
    "for layer in attention_input:\n",
    "    total_attention_as_object = torch.sum(layer, axis=0).tolist()\n",
    "    total_attention_as_object = [att/seq_len for att in total_attention_as_object]\n",
    "    total_attention_as_attender = torch.sum(layer, axis=1).tolist()\n",
    "    total_attention_as_attender = [att/seq_len for att in total_attention_as_attender]\n",
    "    reviced_attention.append(total_attention_as_object)\n",
    "    attender_attention.append(total_attention_as_attender)\n",
    "i = 1\n",
    "for layer in reviced_attention:\n",
    "    recived_pairs.append([(token,att) for token,att in zip(index_tokens,layer)])\n",
    "    print(f'layer{i}:{sorted(recived_pairs[-1],key=lambda x:x[1],reverse=True)[:10]}')\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviced_attention_tensor = [np.array(layer) for layer in reviced_attention]\n",
    "bias_vector = np.ones(reviced_attention_tensor[0].shape[0])\n",
    "bias_vector = bias_vector[None, :]\n",
    "reviced_attention_tensor = np.array(reviced_attention_tensor) + bias_vector\n",
    "reviced_attention_tensor = rmsnorm(torch.tensor(reviced_attention_tensor)).numpy()   \n",
    "layers = reviced_attention_tensor.shape[0]\n",
    "attention_score = np.zeros(reviced_attention_tensor.shape)\n",
    "attention_score[0] = reviced_attention_tensor[0]\n",
    "for i in np.arange(1, layers):\n",
    "    # Perform element-wise multiplication. The original formula is used for calculating an attention matrix [seq_len, seq_len],\n",
    "    # but here it's applied to an attention vector [1, seq_len], so element-wise multiplication is required.\n",
    "    attention_score[i] = reviced_attention_tensor[i] * attention_score[i-1]  \n",
    "last_layer_self_rollout = attention_score[-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reforme the output attention matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gen Step:',type(generated_ids['attentions']),len(generated_ids['attentions']))\n",
    "print('Layers in each step：',type(generated_ids['attentions'][0]),len(generated_ids['attentions'][0]))\n",
    "print('Att matrix in each step:',type(generated_ids['attentions'][0][0]),generated_ids['attentions'][0][0].shape)\n",
    "# Applicable to attention weights without shape transformation, formatted as shown above.\n",
    "if 'llama' in model_id.lower():\n",
    "    all_attentions =  [(t,a) for t,a in zip(response_ids.tolist(), generated_ids['attentions'])] #attention dict for all tokens\n",
    "    all_attentions_avg = [(t,sum(a)/len(a)) for t,a in zip(response_ids.tolist(), generated_ids['attentions'])] #average attention dict for all tokens\n",
    "elif 'qwen' in model_id.lower():\n",
    "    \n",
    "    all_attentions =  [(t,a) for t,a in zip(response_ids[0].tolist(), generated_ids['attentions'])]\n",
    "    all_attentions_avg = [(t,sum(a)/len(a)) for t,a in zip(response_ids[0].tolist(), generated_ids['attentions'])] #average attention dict for all tokens\n",
    "print(all_attentions_avg[0][1].shape)\n",
    "for i in range(len(generated_ids['attentions'])):\n",
    "    for j in range(len(generated_ids['attentions'][i])):\n",
    "        print(f'Gen Step {i+1},Layer {j+1}',generated_ids['attentions'][i][j].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reforme the 1st step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions_step1 = generated_ids['attentions'][0]\n",
    "\n",
    "# Create a list to store the last token attention matrix for each layer\n",
    "last_token_attention_list = []\n",
    "assert generated_ids[0][0][end_index:end_index+1] == tokenizer.eos_token_id\n",
    "# Iterate through each layer's attention matrix\n",
    "for layer_attention in attentions_step1:\n",
    "    # Choose the last token's attention weights\n",
    "    last_token_attention = layer_attention[:, :, end_index:end_index+1:, :]\n",
    "    last_token_attention_list.append(last_token_attention)\n",
    "\n",
    "\n",
    "# Show the shape of the last token attention matrix for the first layer\n",
    "print(last_token_attention_list[0].shape)\n",
    "print(len(last_token_attention_list))\n",
    "attentions_list = list(generated_ids['attentions'])\n",
    "\n",
    "# Convert the list of last token attention matrices to a tuple\n",
    "attentions_list[0] = tuple(last_token_attention_list)\n",
    "\n",
    "# Convert the list back to a tuple and assign it to generated_ids['attentions']\n",
    "generated_ids['attentions'] = tuple(attentions_list)\n",
    "\n",
    "# Verify the shape of the attention matrix for the first layer\n",
    "print(type(generated_ids['attentions']))  # Shoud be tuple\n",
    "print(generated_ids['attentions'][0][0].shape)  # Should be [batch_size, attention_head, 1, seq_len+1]\n",
    "\n",
    "\n",
    "# Create a list to store the converted attention matrices\n",
    "converted_attentions = []\n",
    "#这里的start_index和end_index代表的是输入序列的起始和结束位置\n",
    "\n",
    "for i in range(len(generated_ids['attentions'])):\n",
    "    # Concatenate the tensors from 28 layers in each generation step, each with shape \n",
    "    # [batch_size = 1, attention_head , seq_len or 1, seq_len or seq_len + generation steps], \n",
    "    # into a single tensor with shape [layer, attention_head, seq_len or 1, seq_len or seq_len + generation steps].\n",
    "    step_attentions = torch.cat([layer for layer in generated_ids['attentions'][i]], dim=0)\n",
    "    # Retain the content between start_index and end_index\n",
    "    if step_attentions.shape[-1] > start_index:\n",
    "        step_attentions = step_attentions[:, :, :, start_index:end_index]\n",
    "    \n",
    "    converted_attentions.append(step_attentions.detach().clone().cpu().to(torch.float32).numpy())\n",
    "\n",
    "# Print the shape of the converted attention matrices\n",
    "# Should be [layer, attention_head, seq_len or 1, seq_len or seq_len + generation steps]\n",
    "for i, tensor in enumerate(converted_attentions):\n",
    "    print(f\"Shape of tensor at step {i+1}: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Rollout (Gene steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "if 'llama' in model_id.lower():\n",
    "    all_attentions = [(t, a) for t, a in zip(response_ids.tolist(), converted_attentions)]\n",
    "elif 'qwen' in model_id.lower():\n",
    "    all_attentions =  [(t,a) for t,a in zip(response_ids[0].tolist(), converted_attentions)]\n",
    "attention_scores = []\n",
    "for pair in all_attentions:\n",
    "    token = pair[0]\n",
    "    # att_mat = pair[1].mean(axis=1)  # Average attention head weights, resulting in a tensor of shape [layer=28, seq_len=input length or 1, seq_len=input length or input length + generation steps]\n",
    "    att_mat = pair[1].max(axis=1)  # Take the maximum attention head weights, resulting in a tensor of shape [layer=28, seq_len=input length or 1, seq_len=input length or input length + generation steps]\n",
    "    bias_vector = np.ones(pair[1].shape[-1])\n",
    "    bias_vector = bias_vector[None, None, :]\n",
    "    att_mat = att_mat + bias_vector  # Apply residual connection on [layers, 1, seq_len]\n",
    "    att_mat = rmsnorm(torch.tensor(att_mat), eps=eps).numpy()  # Apply RMSNorm normalization\n",
    "    # att_mat = att_mat / att_mat.sum(axis=-1)[..., None]  # Normalize\n",
    "    joint_att = np.zeros(att_mat.shape)\n",
    "    layers = joint_att.shape[0]\n",
    "    joint_att[0] = att_mat[0]\n",
    "    for i in np.arange(1, layers):\n",
    "        # Perform element-wise multiplication. The original formula is for calculating an attention matrix [seq_len, seq_len]\n",
    "        # but here it's applied to an attention vector [1, seq_len], so element-wise multiplication is needed\n",
    "        joint_att[i] = att_mat[i] * joint_att[i-1]  \n",
    "    attention_scores.append((token, joint_att))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the last layer attention across all layers and output to HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_ids = []\n",
    "if 'llama' in model_id.lower():\n",
    "    seq_ids = (model_inputs[0].tolist())[start_index:end_index]\n",
    "elif 'qwen' in model_id.lower():\n",
    "    seq_ids = (model_inputs['input_ids'].tolist())[0][start_index:end_index]\n",
    "seq_tokens = [tokenizer.decode([token]) for token in seq_ids][prompt_length+1:] # Extract the input tokens\n",
    "all_generated_attention_scores_avg = np.array([att[1][:,:,prompt_length+1:] for att in attention_scores]).mean(axis=0) # Calculate the average step attention scores \n",
    "last_layer = all_generated_attention_scores_avg[-1] # Extract the last layer attention scores\n",
    "last_layer_self_rollout = last_layer_self_rollout.reshape(1,-1)[:,prompt_length+1:]\n",
    "last_layer = (alpha * last_layer) + (beta* last_layer_self_rollout)\n",
    "print(last_layer.mean())\n",
    "print(len(seq_tokens))\n",
    "generate_text_with_scores_html(last_layer,seq_tokens,normalize= True,method='z-score',output_path=f'TEST.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cumulative Attention Animation (Work in Progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_avg_attentions = []\n",
    "\n",
    "def list_add(a,b):\n",
    "    return [a[i]+b[i] for i in range(len(a))]\n",
    "\n",
    "# Accumulate the average attention step by step for each generation step\n",
    "for step in range(len(attention_scores)):\n",
    "    # Take the attention from the first step up to step+1\n",
    "    avg_attention = np.array([att[1][:,:,prompt_length+1:] for att in attention_scores[:step+1]]).mean(axis=0)[-1] * beta # multiply by beta\n",
    "    # Add the cumulative average attention of the current step to the list\n",
    "    cumulative_avg_attentions.append(avg_attention)\n",
    "\n",
    "last_layer_self_rollout_ = last_layer_self_rollout.reshape(1, -1)[:,prompt_length+1:]\n",
    "last_layer_self_rollout_ = alpha * last_layer_self_rollout # multiply by alpha\n",
    "\n",
    "#z-score normalization\n",
    "cumulative_avg_attentions = [(att - att.mean()/att.std()).tolist()[0] for att in cumulative_avg_attentions]\n",
    "self_rollout_mean = last_layer_self_rollout_.mean()\n",
    "self_rollout_std = last_layer_self_rollout_.std()\n",
    "last_layer_self_rollout_ = (last_layer_self_rollout_ - self_rollout_mean)/self_rollout_std\n",
    "last_layer_self_rollout_ = last_layer_self_rollout_.tolist()[0]\n",
    "#cumulative_avg_attentions = [list_add(att,last_layer_self_rollout_) for att in cumulative_avg_attentions]\n",
    "\n",
    "\n",
    "if 'llama' in model_id.lower():\n",
    "    response_per_token = [tokenizer.decode([token]) for token in response_ids]\n",
    "elif 'qwen' in model_id.lower():\n",
    "    response_per_token = [tokenizer.decode([token]) for token in response_ids[0]]\n",
    "with open('attention_data.js', 'w') as f:\n",
    "    f.write(f\"const seq_tokens = {json.dumps(seq_tokens)};\\n\")\n",
    "    f.write(f\"const cumulative_avg_attentions = {json.dumps(cumulative_avg_attentions)};\\n\")\n",
    "    f.write(f\"const response_per_token = {json.dumps(response_per_token)};\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
