# A Novel Explainable Framework for Large Language Models via Attention Rollout: A Case Study on Detecting Depression in Social Media

This is a temporary repo for the demo codes

## Currently include implements
llama3-8B-Chinese-Chat, Qwen2-7B-Instruct and glm-4-9b-chat (require copy special tweaks,see notebook)


## Usage
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Raag6cAOOiuyo70hjN2Yz3VPWn7n3NYX?usp=sharing)

## Notes
This demo only outputs the raw decoded token and its attention rollouts, some tokens will show as "ï¿½"
