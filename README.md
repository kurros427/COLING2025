# A Novel Explainable Framework for Large Language Models via Attention Rollout: A Case Study on Detecting Depression in Social Media

This is a temporary repo for the demo codes

## Currently include implements
LLAMA3-8B-Chinese-Chat, QWEN2-7B-Instruct and GLM-4-9b-chat (require  special tweaks,see notebook)


## Usage
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Raag6cAOOiuyo70hjN2Yz3VPWn7n3NYX?usp=sharing)

## Notes
This demo only outputs the raw decoded token and its attention rollouts, some tokens will show as "ï¿½"
