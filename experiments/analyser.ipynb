{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import hanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_pkl_files(directories,exp,sample_type):\n",
    "    if type(directories) == str:\n",
    "        directories = [directories]\n",
    "    for directory in directories:\n",
    "        pkl_files = {exp: {sample: {} for sample in sample_type} for exp in exp}\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                pkl_path = os.path.join(root, file)\n",
    "                # 使用os.path的方法，确保跨平台兼容性\n",
    "                filename = os.path.basename(pkl_path)\n",
    "                user_id = f\"user_{filename.split('_')[-1].split('.')[0]}\"\n",
    "                sample_type = filename.split('_')[0]\n",
    "                experiment = filename.split('_')[1]\n",
    "                if experiment in exp :\n",
    "                    if sample_type in sample_type:\n",
    "                        if user_id not in pkl_files[experiment][sample_type]:\n",
    "                            pkl_files[experiment][sample_type][user_id] = []\n",
    "                            pkl_files[experiment][sample_type][user_id].append(pkl_path)\n",
    "    return pkl_files\n",
    "\n",
    "def find_common_keys(A, B):\n",
    "    # 创建一个空的字典用来存储相同的key\n",
    "    common_keys = {}\n",
    "\n",
    "    # 遍历A的第一层key\n",
    "    for outer_key in A:\n",
    "        # 确保B中也有相同的第一层key\n",
    "        if outer_key in B:\n",
    "            # 遍历A和B的第二层嵌套结构\n",
    "            for inner_key in A[outer_key]:\n",
    "                # 确保B中有相同的第二层key\n",
    "                if inner_key in B[outer_key]:\n",
    "                    # 获取A和B在第二层字典中的key集合\n",
    "                    keys_in_A = set(A[outer_key][inner_key].keys())\n",
    "                    keys_in_B = set(B[outer_key][inner_key].keys())\n",
    "                    \n",
    "                    # 计算两个集合的交集，找出相同的key\n",
    "                    common = keys_in_A & keys_in_B\n",
    "                    \n",
    "                    # 如果有相同的key，将其记录在common_keys中\n",
    "                    if common:\n",
    "                        if outer_key not in common_keys:\n",
    "                            common_keys[outer_key] = {}\n",
    "                        common_keys[outer_key][inner_key] = common\n",
    "\n",
    "    return common_keys\n",
    "\n",
    "def find_common_keys_dicts(dict_list):\n",
    "    # 创建一个空的字典用来存储相同的key\n",
    "    common_keys = {}\n",
    "\n",
    "    # 遍历第一个字典的第一层key\n",
    "    for outer_key in dict_list[0]:\n",
    "        # 确保每个字典都有相同的第一层key\n",
    "        if all(outer_key in d for d in dict_list):\n",
    "            # 遍历每个字典的第二层嵌套结构\n",
    "            for inner_key in dict_list[0][outer_key]:\n",
    "                # 确保每个字典中有相同的第二层key\n",
    "                if all(inner_key in d[outer_key] for d in dict_list):\n",
    "                    # 获取每个字典在第二层中的key集合\n",
    "                    common = set(dict_list[0][outer_key][inner_key].keys())\n",
    "                    \n",
    "                    # 对比每个字典，找出共有的key\n",
    "                    for d in dict_list[1:]:\n",
    "                        common &= set(d[outer_key][inner_key].keys())\n",
    "                    \n",
    "                    # 如果有相同的key，将其记录在common_keys中\n",
    "                    if common:\n",
    "                        if outer_key not in common_keys:\n",
    "                            common_keys[outer_key] = {}\n",
    "                        common_keys[outer_key][inner_key] = common\n",
    "    \n",
    "    return common_keys\n",
    "\n",
    "def generate_text_with_scores_html(tensor, text, output_path, normalize=True, method='min-max', window_size=10):\n",
    "    \"\"\"\n",
    "    根据输入的向量和文本生成一个HTML文件，其中每个字符上方显示对应的分数，\n",
    "    并根据分数调整颜色。最终的HTML文件保存在指定路径中。\n",
    "\n",
    "    参数:\n",
    "    tensor (numpy.ndarray): 形状为 (1, N) 的向量，其中 N 是文本的长度。\n",
    "    text (str): 文本字符串，与向量长度匹配。\n",
    "    output_path (str): 输出HTML文件的路径。\n",
    "    normalize (bool): 是否归一化分数，默认为True。\n",
    "    method (str): 归一化方法，支持 'min-max'（默认）、'mean' 和 'moving-average'。\n",
    "    window_size (int): 滑动平均窗口大小，默认为10，仅在选择滑动平均归一化时使用。\n",
    "\n",
    "    返回:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # 检查张量和文本长度是否匹配\n",
    "    assert tensor.shape[1] == len(text), \"张量和文本的长度不匹配！\"\n",
    "\n",
    "    scores = tensor[0]\n",
    "\n",
    "    # 归一化分数\n",
    "    if normalize:\n",
    "        if method == 'min-max':\n",
    "            scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "        elif method == 'mean':\n",
    "            mean_value = scores.mean()\n",
    "            scores = scores - mean_value\n",
    "        elif method == 'moving-average':\n",
    "            mean_value = np.convolve(scores, np.ones(window_size) / window_size, mode='same')\n",
    "            scores = scores - mean_value\n",
    "        else:\n",
    "            raise ValueError(\"未知的归一化方法。请使用 'min-max', 'mean', 或 'moving-average'。\")\n",
    "\n",
    "    # 创建颜色映射函数\n",
    "    def score_to_color(score):\n",
    "        r = int(255 * score)\n",
    "        b = 255 - r\n",
    "        return f'rgb({r}, 0, {b})'\n",
    "\n",
    "    # 定义每行显示的字符数\n",
    "    chars_per_line = 50\n",
    "    num_lines = len(text) // chars_per_line + (1 if len(text) % chars_per_line else 0)\n",
    "\n",
    "    # 生成HTML内容\n",
    "    html_content = \"<html><body style='font-family:monospace;'>\\n\"\n",
    "\n",
    "    # 设置间隔\n",
    "    spacing = \"20px\"  # 可以根据需要调整间隔\n",
    "\n",
    "    for line in range(num_lines):\n",
    "        start_idx = line * chars_per_line\n",
    "        end_idx = start_idx + chars_per_line\n",
    "        line_text = text[start_idx:end_idx]\n",
    "        line_scores = scores[start_idx:end_idx]\n",
    "\n",
    "        for i, char in enumerate(line_text):\n",
    "            color = score_to_color(line_scores[i])\n",
    "            score_text = f\"{line_scores[i]:.2f}\"\n",
    "            border_style = \"border: 1px solid black;\" if line_scores[i] > 0 else \"\"\n",
    "            html_content += f\"<div style='display:inline-block; text-align:center; color:{color}; margin-right:{spacing}; {border_style}'>\" \\\n",
    "                            f\"<div style='font-size:0.5em;'>{score_text}</div>\" \\\n",
    "                            f\"<div>{char}</div>\" \\\n",
    "                            f\"</div>\"\n",
    "\n",
    "        html_content += \"<br>\\n\"\n",
    "\n",
    "    html_content += \"</body></html>\"\n",
    "\n",
    "    # 保存为HTML文件\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "    print(f\"HTML文件已生成并保存在 {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens_with_error_handling(tokenizer, token_ids):\n",
    "    new_list = []  # 用于存储新的解码结果\n",
    "    i = 0\n",
    "\n",
    "    while i < len(token_ids):\n",
    "        current_token = [token_ids[i]]  # 当前的 token\n",
    "        decoded = tokenizer.decode(current_token)  # 尝试解码当前 token\n",
    "\n",
    "        # 如果解码结果是 \"�\"，开始尝试联合后续 token 进行解码\n",
    "        if \"�\" in decoded:\n",
    "            combined_tokens = [token_ids[i]]  # 当前 token\n",
    "            i += 1\n",
    "            while i < len(token_ids):\n",
    "                combined_tokens.append(token_ids[i])\n",
    "                decoded = tokenizer.decode(combined_tokens)  # 尝试解码组合后的 tokens\n",
    "\n",
    "                # 直到解码结果不再包含 \"�\"，则停止组合\n",
    "                if \"�\" not in decoded:\n",
    "                    new_list.append(combined_tokens)  # 将成功组合解码的 token 添加到新列表\n",
    "                    break\n",
    "                i += 1\n",
    "\n",
    "            # 如果到最后仍然是 \"�\"，也添加组合的 token 列表\n",
    "            if \"�\" in decoded:\n",
    "                new_list.append(combined_tokens)\n",
    "        else:\n",
    "            # 如果解码成功，直接添加当前 token\n",
    "            new_list.append(token_ids[i])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return new_list\n",
    "\n",
    "def decode_tokens_with_att_handling(tokenizer, token_ids, att):\n",
    "    new_token_list = []  # 用于存储新的解码后的 token 列表\n",
    "    new_att_list = []  # 用于存储新的 att 列表\n",
    "    i = 0\n",
    "\n",
    "    while i < len(token_ids):\n",
    "        current_token = [token_ids[i]]  # 当前的 token\n",
    "        decoded = tokenizer.decode(current_token)  # 尝试解码当前 token\n",
    "\n",
    "        # 如果解码结果是 \"�\"，开始尝试联合后续 token 进行解码\n",
    "        if \"�\" in decoded:\n",
    "            combined_tokens = [token_ids[i]]  # 当前 token\n",
    "            combined_att = [att[i]]  # 对应的 att\n",
    "            i += 1\n",
    "            while i < len(token_ids):\n",
    "                combined_tokens.append(token_ids[i])\n",
    "                combined_att.append(att[i])  # 合并 att 列表中的元素\n",
    "                decoded = tokenizer.decode(combined_tokens)  # 尝试解码组合后的 tokens\n",
    "\n",
    "                # 直到解码结果不再包含 \"�\"，则停止组合\n",
    "                if \"�\" not in decoded:\n",
    "                    new_token_list.append(combined_tokens)  # 将成功组合解码的 token 添加到新列表\n",
    "                    new_att_list.append(combined_att)  # 将合并的 att 添加到新列表\n",
    "                    break\n",
    "                i += 1\n",
    "\n",
    "            # 如果到最后仍然是 \"�\"，也添加组合的 token 列表\n",
    "            if \"�\" in decoded:\n",
    "                new_token_list.append(combined_tokens)\n",
    "                new_att_list.append(combined_att)\n",
    "        else:\n",
    "            # 如果解码成功，直接添加当前 token 和对应的 att\n",
    "            new_token_list.append(token_ids[i])\n",
    "            new_att_list.append(att[i])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return new_token_list, new_att_list\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "def clean_up_tuples(tuple_list):\n",
    "    processed_list = []\n",
    "    removed_list = []\n",
    "    # Define a regex pattern to match only Chinese characters\n",
    "    chinese_pattern = re.compile(r'[\\u4E00-\\u9FFF\\u3400-\\u4DBF\\uF900-\\uFAFF]+')\n",
    "    \n",
    "    for item in tuple_list:\n",
    "        # Process the first element of the tuple\n",
    "        cleaned_token = item[0].replace('\\n', '').replace(' ', '').replace('️', '')\n",
    "\n",
    "        # Remove all non-printing, special characters, punctuation, and symbols\n",
    "        cleaned_token = ''.join(\n",
    "            char for char in cleaned_token \n",
    "            if chinese_pattern.match(char)\n",
    "        )\n",
    "        \n",
    "        # Check if the cleaned token is valid: must not be empty\n",
    "        if cleaned_token:\n",
    "            # Add the cleaned token with the original value to the processed list\n",
    "            processed_list.append((cleaned_token, item[1]))\n",
    "        else:\n",
    "            removed_list.append((item[0], item[1]))\n",
    "\n",
    "    return processed_list, removed_list\n",
    "\n",
    "\n",
    "def clean_text_list(text_list):\n",
    "    # Define a regex pattern to match only Chinese characters\n",
    "    chinese_pattern = re.compile(r'[\\u4E00-\\u9FFF\\u3400-\\u4DBF\\uF900-\\uFAFF]+')\n",
    "    \n",
    "    # Define extended punctuation set including English and Chinese punctuation marks\n",
    "    extended_punctuation = string.punctuation + '《》（）【】、；：。，！？「」『』“”‘’'\n",
    "    punctuation_pattern = re.compile(f\"[{re.escape(extended_punctuation)}]\")\n",
    "    \n",
    "    cleaned_list = []\n",
    "    for text in text_list:\n",
    "        # Remove punctuation, spaces, newline characters, and special characters\n",
    "        cleaned_text = punctuation_pattern.sub('', text.replace(' ', '').replace('\\n', '').replace('️', ''))\n",
    "        \n",
    "        # Keep only Chinese characters\n",
    "        cleaned_text = ''.join(chinese_pattern.findall(cleaned_text))\n",
    "        \n",
    "        # Add to cleaned list if the result is not empty\n",
    "        if cleaned_text:\n",
    "            cleaned_list.append(cleaned_text)\n",
    "    \n",
    "    return cleaned_list\n",
    "import numpy as np\n",
    "\n",
    "def match_and_average_complex(tk_at, l_new_token):\n",
    "    l_new_token_attention = []\n",
    "    index_tk_at = 0\n",
    "    i = 0\n",
    "\n",
    "    while i < len(l_new_token) and index_tk_at < len(tk_at):\n",
    "        # 创建当前需要匹配的l_new_token token\n",
    "        current_l_token = l_new_token[i]#有\n",
    "        current_tk_token, current_value = tk_at[index_tk_at]\n",
    "        current_tk_combined = current_tk_token#有什么\n",
    "        current_values = [current_value]\n",
    "\n",
    "        while True:\n",
    "            if len(current_tk_combined) < len(current_l_token) and index_tk_at + 1 < len(tk_at):\n",
    "                index_tk_at += 1\n",
    "                current_tk_combined += tk_at[index_tk_at][0]\n",
    "                current_values.append(tk_at[index_tk_at][1])\n",
    "                #print('1: ',current_tk_combined,current_l_token)\n",
    "            if len(current_tk_combined) > len(current_l_token) and i + 1 < len(l_new_token):#有什么 #有\n",
    "                i += 1\n",
    "                current_l_token += l_new_token[i]#有 什么样\n",
    "                current_values.append(tk_at[index_tk_at][1])\n",
    "                #print('2: ',current_tk_combined,current_l_token)\n",
    "            if current_tk_combined == current_l_token: #两个匹配的完全一样的情况\n",
    "                averaged_value = np.mean(current_values)\n",
    "                tmp = (current_tk_combined,averaged_value)\n",
    "                l_new_token_attention.append(tmp)\n",
    "                index_tk_at += 1\n",
    "                i += 1\n",
    "                #print('3: ',current_tk_combined,current_l_token)\n",
    "                break\n",
    "    \n",
    "    return l_new_token_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen   = find_all_pkl_files('E:\\qwen\\qwen',exps,sample_types)\n",
    "llama = find_all_pkl_files('E:\\llama\\llama',exps,sample_types)\n",
    "glm = find_all_pkl_files('E:\\glm\\glm',exps,sample_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    }
   ],
   "source": [
    "common_users = find_common_keys_dicts([qwen,llama,glm])\n",
    "HanLP = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {'qwen':qwen}#,'llama':llama,'glm':glm,'mind':mind}\n",
    "\n",
    "\n",
    "\n",
    "for model in models :\n",
    "    if model == 'qwen':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\n",
    "        qwen_result = {exp: {sample: {} for sample in sample_types} for exp in exps}\n",
    "    elif model == 'llama':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"D:\\LLMs\\Llama3-8B-Chinese-Chat\")\n",
    "        llama_result = {exp: {sample: {} for sample in sample_types} for exp in exps}\n",
    "    elif model == 'glm':\n",
    "        tokenizer = AutoTokenizer.from_pretrained('D:\\LLMs\\glm-4-9b-chat',trust_remote_code=True)\n",
    "        glm_result = {exp: {sample: {} for sample in sample_types} for exp in exps}\n",
    "    elif model == 'mind':\n",
    "        tokenizer = AutoTokenizer.from_pretrained('D:\\LLMs\\MindChat-Qwen-7B-v2',trust_remote_code=True)\n",
    "        mind_result = {exp: {sample: {} for sample in sample_types} for exp in exps}\n",
    "    elif model == 'mc':\n",
    "        tokenizer = AutoTokenizer.from_pretrained('D:\\LLMs\\MeChat',trust_remote_code=True)\n",
    "        mc_result = {exp: {sample: {} for sample in sample_types} for exp in exps}\n",
    "    for experiment in common_users:\n",
    "        for sample_type in common_users[experiment]:\n",
    "            users = common_users[experiment][sample_type]\n",
    "            for user in users:\n",
    "                if model == 'qwen':\n",
    "                    qwen_result[experiment][sample_type][user] = {}\n",
    "                elif model == 'llama':\n",
    "                    llama_result[experiment][sample_type][user] = {}\n",
    "                elif model == 'glm':\n",
    "                    glm_result[experiment][sample_type][user] = {}\n",
    "                elif model == 'mind':\n",
    "                    mind_result[experiment][sample_type][user] = {}\n",
    "                elif model == 'mc':\n",
    "                    mc_result[experiment][sample_type][user] = {}\n",
    "                #print(model,experiment,sample_type,user)\n",
    "                with open(models[model][experiment][sample_type][user][0], 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                if model == 'qwen':\n",
    "                    joint_attentions = data['joint_attentions']\n",
    "                    model_inputs = data['model_inputs'].input_ids[0]\n",
    "                    self_joint_attentions = data['self_joint_attention']\n",
    "                    prompt_drift = {'OR2-REV': 67,'OR2': 78}\n",
    "                    start = 0\n",
    "                    end = 0\n",
    "                    for i,token in enumerate(model_inputs):\n",
    "                        decoded = tokenizer.decode(token)\n",
    "                        #print(i, decoded)\n",
    "                        if decoded == 'system':\n",
    "                            start = i+2\n",
    "                        if decoded == tokenizer.eos_token:\n",
    "                            end = i\n",
    "                            break\n",
    "                    start = start + prompt_drift[experiment]\n",
    "                    #seq_tokens = [tokenizer.decode([token]) for token in model_inputs[q_start:q_end]]\n",
    "                    all_generated_joint_attentions_avg_last = np.array([att[1][:,:,prompt_drift[experiment]:end] for att in joint_attentions]).mean(axis=0)[-1] #对生成步骤求平均，取最后一个，[1,input_len]\n",
    "                    last_self_attention = self_joint_attentions[-1][prompt_drift[experiment]:].reshape(1,-1) #取最后一层的自注意力 [1,input_len]\n",
    "                    \n",
    "                    all_generated_joint_attentions_avg_last_token, all_generated_joint_attentions_avg_last_att = decode_tokens_with_att_handling(tokenizer,data['model_inputs'].input_ids[0][start:end],all_generated_joint_attentions_avg_last[0].tolist())\n",
    "                    last_self_attention_token, last_self_attention_att = decode_tokens_with_att_handling(tokenizer,data['model_inputs'].input_ids[0][start:end],last_self_attention.tolist()[0])\n",
    "                    all_generated_joint_attentions_avg_last_att = [\n",
    "                            sum(att) / len(att) if isinstance(att, list) else att\n",
    "                            for att in all_generated_joint_attentions_avg_last_att\n",
    "                        ]\n",
    "                    #assert len(all_generated_joint_attentions_avg_last_token) == len(all_generated_joint_attentions_avg_last_att)\n",
    "                    last_self_attention_att = [\n",
    "                            sum(att) / len(att) if isinstance(att, list) else att\n",
    "                            for att in last_self_attention_att\n",
    "                        ]\n",
    "                    assert len(all_generated_joint_attentions_avg_last_token) == len(last_self_attention_token)\n",
    "                    assert all_generated_joint_attentions_avg_last_token == last_self_attention_token\n",
    "                    joint_att_pairs = [(tokenizer.decode(token), att) for token, att in zip(all_generated_joint_attentions_avg_last_token, all_generated_joint_attentions_avg_last_att)]\n",
    "                    self_att_pairs = [(tokenizer.decode(token), att) for token, att in zip(last_self_attention_token, last_self_attention_att)]\n",
    "                    qwen_result[experiment][sample_type][user]['join_att_before_seg'] = joint_att_pairs\n",
    "                    qwen_result[experiment][sample_type][user]['self_att_before_seg'] = self_att_pairs\n",
    "                    l_new_token = HanLP(''.join([tokenizer.decode(token) for token in all_generated_joint_attentions_avg_last_token]))\n",
    "                    l_new_token_cleaned = clean_text_list(l_new_token)\n",
    "                    cleaned_joint_att_paris, removed_joint_att_paris = clean_up_tuples(joint_att_pairs)\n",
    "                    cleaned_self_att_paris, removed_self_att_paris = clean_up_tuples(self_att_pairs)\n",
    "                    assert len(cleaned_joint_att_paris) == len(cleaned_self_att_paris)\n",
    "                    assert len(removed_joint_att_paris) == len(removed_self_att_paris)\n",
    "                    seged_joint_att_pairs = match_and_average_complex(cleaned_joint_att_paris, l_new_token_cleaned)\n",
    "                    seged_self_att_pairs = match_and_average_complex(cleaned_self_att_paris, l_new_token_cleaned)\n",
    "                    qwen_result[experiment][sample_type][user]['join_att_after_seg'] = seged_joint_att_pairs\n",
    "                    qwen_result[experiment][sample_type][user]['self_att_after_seg'] = seged_self_att_pairs\n",
    "                    qwen_result[experiment][sample_type][user]['removed_joint_att_paris'] = removed_joint_att_paris\n",
    "                    qwen_result[experiment][sample_type][user]['removed_self_att_paris'] = removed_self_att_paris\n",
    "                elif model == 'llama':\n",
    "                    joint_attentions = data['joint_attentions']\n",
    "                    model_inputs = data['model_inputs'][0]\n",
    "                    self_joint_attentions = data['self_joint_attention']\n",
    "                    prompt_drift = {'OR2-REV': 84,'OR2': 91}\n",
    "                    start = 0\n",
    "                    end = 0\n",
    "                    for i,token in enumerate(model_inputs):\n",
    "                        decoded = tokenizer.decode(token)\n",
    "                        #print(i, decoded)\n",
    "                        if decoded == 'system':\n",
    "                            start = i+3\n",
    "                        if decoded == tokenizer.eos_token:\n",
    "                            end = i\n",
    "                            break\n",
    "                    start = start + prompt_drift[experiment]\n",
    "                    #seq_tokens = [tokenizer.decode([token]) for token in model_inputs[q_start:q_end]]\n",
    "                    all_generated_joint_attentions_avg_last = np.array([att[1][:,:,prompt_drift[experiment]:end] for att in joint_attentions]).mean(axis=0)[-1] #对生成步骤求平均，取最后一个，[1,input_len]\n",
    "                    last_self_attention = self_joint_attentions[-1][prompt_drift[experiment]:].reshape(1,-1)\n",
    "                    #assert len(all_generated_joint_attentions_avg_last) == len(last_self_attention[0])\n",
    "                    all_generated_joint_attentions_avg_last_token, all_generated_joint_attentions_avg_last_att = decode_tokens_with_att_handling(tokenizer,data['model_inputs'][0][start:end],all_generated_joint_attentions_avg_last[0].tolist())\n",
    "                    last_self_attention_token, last_self_attention_att = decode_tokens_with_att_handling(tokenizer,data['model_inputs'][0][start:end],last_self_attention.tolist()[0])\n",
    "                    all_generated_joint_attentions_avg_last_att = [\n",
    "                            sum(att) / len(att) if isinstance(att, list) else att\n",
    "                            for att in all_generated_joint_attentions_avg_last_att\n",
    "                        ]\n",
    "                    #assert len(all_generated_joint_attentions_avg_last_token) == len(all_generated_joint_attentions_avg_last_att)\n",
    "                    last_self_attention_att = [\n",
    "                            sum(att) / len(att) if isinstance(att, list) else att\n",
    "                            for att in last_self_attention_att\n",
    "                        ]\n",
    "                    assert len(all_generated_joint_attentions_avg_last_token) == len(last_self_attention_token)\n",
    "                    assert all_generated_joint_attentions_avg_last_token == last_self_attention_token\n",
    "                    joint_att_pairs = [(tokenizer.decode(token), att) for token, att in zip(all_generated_joint_attentions_avg_last_token, all_generated_joint_attentions_avg_last_att)]\n",
    "                    self_att_pairs = [(tokenizer.decode(token), att) for token, att in zip(last_self_attention_token, last_self_attention_att)]\n",
    "                    llama_result[experiment][sample_type][user]['join_att_before_seg'] = joint_att_pairs\n",
    "                    llama_result[experiment][sample_type][user]['self_att_before_seg'] = self_att_pairs\n",
    "                    l_new_token = HanLP(''.join([tokenizer.decode(token) for token in all_generated_joint_attentions_avg_last_token]))\n",
    "                    l_new_token_cleaned = clean_text_list(l_new_token)\n",
    "                    cleaned_joint_att_paris, removed_joint_att_paris = clean_up_tuples(joint_att_pairs)\n",
    "                    cleaned_self_att_paris, removed_self_att_paris = clean_up_tuples(self_att_pairs)\n",
    "                    assert len(cleaned_joint_att_paris) == len(cleaned_self_att_paris)\n",
    "                    assert len(removed_joint_att_paris) == len(removed_self_att_paris)\n",
    "                    seged_joint_att_pairs = match_and_average_complex(cleaned_joint_att_paris, l_new_token_cleaned)\n",
    "                    seged_self_att_pairs = match_and_average_complex(cleaned_self_att_paris, l_new_token_cleaned)\n",
    "                    llama_result[experiment][sample_type][user]['join_att_after_seg'] = seged_joint_att_pairs\n",
    "                    llama_result[experiment][sample_type][user]['self_att_after_seg'] = seged_self_att_pairs\n",
    "                    llama_result[experiment][sample_type][user]['removed_joint_att_paris'] = removed_joint_att_paris\n",
    "                    llama_result[experiment][sample_type][user]['removed_self_att_paris'] = removed_self_att_paris\n",
    "                elif model == 'glm':\n",
    "                    glm_result[experiment][sample_type][user] = {}\n",
    "                    tokenizer = AutoTokenizer.from_pretrained('D:\\LLMs\\glm-4-9b-chat',trust_remote_code=True)\n",
    "                    joint_attentions = data['joint_attentions']\n",
    "                    model_inputs = data['model_inputs'].input_ids[0]\n",
    "                    self_joint_attentions = data['self_joint_attention']\n",
    "                    prompt_drift = {'OR2': 74}\n",
    "                    start = 0\n",
    "                    end = 0\n",
    "                    for i,token in enumerate(model_inputs):\n",
    "                        decoded = tokenizer.decode([token])\n",
    "                        #print(i, decoded)\n",
    "                        if decoded == '<|system|>':\n",
    "                            start = i+2\n",
    "                        if decoded == '<|assistant|>':\n",
    "                            end = i\n",
    "                            break\n",
    "                    start = start + prompt_drift[experiment]\n",
    "                    #seq_tokens = [tokenizer.decode([token]) for token in model_inputs[q_start:q_end]]\n",
    "                    all_generated_joint_attentions_avg_last = np.array([att[1][:,:,prompt_drift[experiment]:end] for att in joint_attentions]).mean(axis=0)[-1]\n",
    "                    last_self_attention = self_joint_attentions[-1][prompt_drift[experiment]:].reshape(1,-1)\n",
    "                    #assert len(all_generated_joint_attentions_avg_last) == len(last_self_attention[0])\n",
    "                    all_generated_joint_attentions_avg_last_token, all_generated_joint_attentions_avg_last_att = decode_tokens_with_att_handling(tokenizer,data['model_inputs'].input_ids[0][start:end],all_generated_joint_attentions_avg_last[0].tolist())\n",
    "                    last_self_attention_token, last_self_attention_att = decode_tokens_with_att_handling(tokenizer,data['model_inputs'].input_ids[0][start:end],last_self_attention.tolist()[0])\n",
    "                    all_generated_joint_attentions_avg_last_att = [\n",
    "                            sum(att) / len(att) if isinstance(att, list) else att\n",
    "                            for att in all_generated_joint_attentions_avg_last_att\n",
    "                        ]\n",
    "                    #assert len(all_generated_joint_attentions_avg_last_token) == len(all_generated_joint_attentions_avg_last_att)\n",
    "                    #print(len(all_generated_joint_attentions_avg_last_token),len(all_generated_joint_attentions_avg_last_att))\n",
    "                    last_self_attention_att = [\n",
    "                            sum(att) / len(att) if isinstance(att, list) else att\n",
    "                            for att in last_self_attention_att\n",
    "                        ]\n",
    "                    assert len(all_generated_joint_attentions_avg_last_token) == len(last_self_attention_token)\n",
    "                    assert all_generated_joint_attentions_avg_last_token == last_self_attention_token\n",
    "                    joint_att_pairs = [\n",
    "                            (tokenizer.decode([token]) if not isinstance(token, list) else tokenizer.decode(token), att)\n",
    "                            for token, att in zip(all_generated_joint_attentions_avg_last_token, all_generated_joint_attentions_avg_last_att)\n",
    "                        ]\n",
    "                    self_att_pairs = [\n",
    "                            (tokenizer.decode([token]) if not isinstance(token, list) else tokenizer.decode(token), att)\n",
    "                            for token, att in zip(last_self_attention_token, last_self_attention_att)\n",
    "                        ]\n",
    "                    glm_result[experiment][sample_type][user]['join_att_before_seg'] = joint_att_pairs\n",
    "                    glm_result[experiment][sample_type][user]['self_att_before_seg'] = self_att_pairs\n",
    "                    token_list = [\n",
    "                            tokenizer.decode([token]) if not isinstance(token, list) else tokenizer.decode(token)\n",
    "                            for token in all_generated_joint_attentions_avg_last_token\n",
    "                        ]\n",
    "                    l_new_token = HanLP(''.join(token_list))\n",
    "                    l_new_token_cleaned = clean_text_list(l_new_token)\n",
    "                    cleaned_joint_att_paris, removed_joint_att_paris = clean_up_tuples(joint_att_pairs)\n",
    "                    cleaned_self_att_paris, removed_self_att_paris = clean_up_tuples(self_att_pairs)\n",
    "                    assert len(cleaned_joint_att_paris) == len(cleaned_self_att_paris)\n",
    "                    assert len(removed_joint_att_paris) == len(removed_self_att_paris)\n",
    "                    seged_joint_att_pairs = match_and_average_complex(cleaned_joint_att_paris, l_new_token_cleaned)\n",
    "                    seged_self_att_pairs = match_and_average_complex(cleaned_self_att_paris, l_new_token_cleaned)\n",
    "                    glm_result[experiment][sample_type][user]['join_att_after_seg'] = seged_joint_att_pairs\n",
    "                    glm_result[experiment][sample_type][user]['self_att_after_seg'] = seged_self_att_pairs\n",
    "                    glm_result[experiment][sample_type][user]['removed_joint_att_paris'] = removed_joint_att_paris\n",
    "                    glm_result[experiment][sample_type][user]['removed_self_att_paris'] = removed_self_att_paris\n",
    "                elif model == 'mind':\n",
    "                    joint_attentions = data['joint_attentions']\n",
    "                    model_inputs = data['model_inputs'].input_ids[0]\n",
    "                    self_joint_attentions = data['self_joint_attention']\n",
    "                    prompt_drift = {'OR2': 34}\n",
    "                    start = 0\n",
    "                    end = 0\n",
    "                    for i,token in enumerate(model_inputs):\n",
    "                        decoded = tokenizer.decode(token)\n",
    "                        #print(i, decoded)\n",
    "                        if decoded == 'user' or decoded == 'system':\n",
    "                            start = i+2\n",
    "                        if decoded ==  '<|im_end|>':\n",
    "                            end = i\n",
    "                            break\n",
    "                    start = start + prompt_drift[experiment]\n",
    "                    #print(start,end)\n",
    "                    #seq_tokens = [tokenizer.decode([token]) for token in model_inputs[q_start:q_end]]\n",
    "                    all_generated_joint_attentions_avg_last = np.array([att[1][:,:,prompt_drift[experiment]:end] for att in joint_attentions]).mean(axis=0)[-1]\n",
    "                    last_self_attention = self_joint_attentions[-1][prompt_drift[experiment]:].reshape(1,-1)\n",
    "                    #assert len(all_generated_joint_attentions_avg_last) == len(last_self_attention[0])\n",
    "                    all_generated_joint_attentions_avg_last_token, all_generated_joint_attentions_avg_last_att = decode_tokens_with_att_handling(tokenizer,data['model_inputs'].input_ids[0][start:end],all_generated_joint_attentions_avg_last[0].tolist())\n",
    "                    last_self_attention_token, last_self_attention_att = decode_tokens_with_att_handling(tokenizer,data['model_inputs'].input_ids[0][start:end],last_self_attention.tolist()[0])\n",
    "                    all_generated_joint_attentions_avg_last_att = [\n",
    "                            sum(att) / len(att) if isinstance(att, list) else att\n",
    "                            for att in all_generated_joint_attentions_avg_last_att\n",
    "                        ]\n",
    "                    #assert len(all_generated_joint_attentions_avg_last_token) == len(all_generated_joint_attentions_avg_last_att)\n",
    "                    last_self_attention_att = [\n",
    "                            sum(att) / len(att) if isinstance(att, list) else att\n",
    "                            for att in last_self_attention_att\n",
    "                        ]\n",
    "                    assert len(all_generated_joint_attentions_avg_last_token) == len(last_self_attention_token)\n",
    "                    assert all_generated_joint_attentions_avg_last_token == last_self_attention_token\n",
    "                    joint_att_pairs = [(tokenizer.decode(token), att) for token, att in zip(all_generated_joint_attentions_avg_last_token, all_generated_joint_attentions_avg_last_att)]\n",
    "                    self_att_pairs = [(tokenizer.decode(token), att) for token, att in zip(last_self_attention_token, last_self_attention_att)]\n",
    "                    mind_result[experiment][sample_type][user]['join_att_before_seg'] = joint_att_pairs\n",
    "                    mind_result[experiment][sample_type][user]['self_att_before_seg'] = self_att_pairs\n",
    "                    l_new_token = HanLP(''.join([tokenizer.decode(token) for token in all_generated_joint_attentions_avg_last_token]))\n",
    "                    l_new_token_cleaned = clean_text_list(l_new_token)\n",
    "                    cleaned_joint_att_paris, removed_joint_att_paris = clean_up_tuples(joint_att_pairs)\n",
    "                    cleaned_self_att_paris, removed_self_att_paris = clean_up_tuples(self_att_pairs)\n",
    "                    assert len(cleaned_joint_att_paris) == len(cleaned_self_att_paris)\n",
    "                    assert len(removed_joint_att_paris) == len(removed_self_att_paris)\n",
    "                    seged_joint_att_pairs = match_and_average_complex(cleaned_joint_att_paris, l_new_token_cleaned)\n",
    "                    seged_self_att_pairs = match_and_average_complex(cleaned_self_att_paris, l_new_token_cleaned)\n",
    "                    mind_result[experiment][sample_type][user]['join_att_after_seg'] = seged_joint_att_pairs\n",
    "                    mind_result[experiment][sample_type][user]['self_att_after_seg'] = seged_self_att_pairs\n",
    "                    mind_result[experiment][sample_type][user]['removed_joint_att_paris'] = removed_joint_att_paris\n",
    "                    mind_result[experiment][sample_type][user]['removed_self_att_paris'] = removed_self_att_paris\n",
    "                elif model == 'mc':\n",
    "                    joint_attentions = data['joint_attentions']\n",
    "                    model_inputs = data['model_inputs'].input_ids[0]\n",
    "                    self_joint_attentions = data['self_joint_attention']\n",
    "                    prompt_drift = {'OR2': 35}\n",
    "                    start = 0\n",
    "                    end = 0\n",
    "                    for i,token in enumerate(model_inputs):\n",
    "                        decoded = tokenizer.decode(token)\n",
    "                        #print(i, decoded)\n",
    "                        if decoded == '问' and start == 0:\n",
    "                            start = i+2\n",
    "                    end = len(data['model_inputs'].input_ids[0])-5\n",
    "                    start = start + prompt_drift[experiment]\n",
    "                    #print(start,end)\n",
    "                    #seq_tokens = [tokenizer.decode([token]) for token in model_inputs[q_start:q_end]]\n",
    "                    all_generated_joint_attentions_avg_last = np.array([att[1][:,:,prompt_drift[experiment]:end] for att in joint_attentions]).mean(axis=0)[-1]\n",
    "                    last_self_attention = self_joint_attentions[-1][prompt_drift[experiment]:].reshape(1,-1)\n",
    "                    #assert len(all_generated_joint_attentions_avg_last) == len(last_self_attention[0])\n",
    "                    all_generated_joint_attentions_avg_last_token, all_generated_joint_attentions_avg_last_att = decode_tokens_with_att_handling(tokenizer,data['model_inputs'].input_ids[0][start:end],all_generated_joint_attentions_avg_last[0].tolist())\n",
    "                    last_self_attention_token, last_self_attention_att = decode_tokens_with_att_handling(tokenizer,data['model_inputs'].input_ids[0][start:end],last_self_attention.tolist()[0])\n",
    "                    all_generated_joint_attentions_avg_last_att = [\n",
    "                            sum(att) / len(att) if isinstance(att, list) else att\n",
    "                            for att in all_generated_joint_attentions_avg_last_att\n",
    "                        ]\n",
    "                    #assert len(all_generated_joint_attentions_avg_last_token) == len(all_generated_joint_attentions_avg_last_att)\n",
    "                    last_self_attention_att = [\n",
    "                            sum(att) / len(att) if isinstance(att, list) else att\n",
    "                            for att in last_self_attention_att\n",
    "                        ]\n",
    "                    assert len(all_generated_joint_attentions_avg_last_token) == len(last_self_attention_token)\n",
    "                    assert all_generated_joint_attentions_avg_last_token == last_self_attention_token\n",
    "                    joint_att_pairs = [\n",
    "                            (tokenizer.decode([token]) if not isinstance(token, list) else tokenizer.decode(token), att)\n",
    "                            for token, att in zip(all_generated_joint_attentions_avg_last_token, all_generated_joint_attentions_avg_last_att)\n",
    "                        ]\n",
    "                    self_att_pairs = [\n",
    "                            (tokenizer.decode([token]) if not isinstance(token, list) else tokenizer.decode(token), att)\n",
    "                            for token, att in zip(last_self_attention_token, last_self_attention_att)\n",
    "                        ]\n",
    "                    mc_result[experiment][sample_type][user]['join_att_before_seg'] = joint_att_pairs\n",
    "                    mc_result[experiment][sample_type][user]['self_att_before_seg'] = self_att_pairs\n",
    "                    token_list = [\n",
    "                            tokenizer.decode([token]) if not isinstance(token, list) else tokenizer.decode(token)\n",
    "                            for token in all_generated_joint_attentions_avg_last_token\n",
    "                        ]\n",
    "                    l_new_token = HanLP(''.join(token_list))\n",
    "                    l_new_token_cleaned = clean_text_list(l_new_token)\n",
    "                    cleaned_joint_att_paris, removed_joint_att_paris = clean_up_tuples(joint_att_pairs)\n",
    "                    cleaned_self_att_paris, removed_self_att_paris = clean_up_tuples(self_att_pairs)\n",
    "                    assert len(cleaned_joint_att_paris) == len(cleaned_self_att_paris)\n",
    "                    assert len(removed_joint_att_paris) == len(removed_self_att_paris)\n",
    "                    seged_joint_att_pairs = match_and_average_complex(cleaned_joint_att_paris, l_new_token_cleaned)\n",
    "                    seged_self_att_pairs = match_and_average_complex(cleaned_self_att_paris, l_new_token_cleaned)\n",
    "                    mc_result[experiment][sample_type][user]['join_att_after_seg'] = seged_joint_att_pairs\n",
    "                    mc_result[experiment][sample_type][user]['self_att_after_seg'] = seged_self_att_pairs\n",
    "                    mc_result[experiment][sample_type][user]['removed_joint_att_paris'] = removed_joint_att_paris\n",
    "                    mc_result[experiment][sample_type][user]['removed_self_att_paris'] = removed_self_att_paris\n",
    "\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "qwen_result_analysis = {}\n",
    "for user in qwen_result['OR2']['positive']:\n",
    "    join_att_after_seg = qwen_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = qwen_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    z_scores_joint_att = zscore (join_att)\n",
    "    z_scores_self_att = zscore (self_att)\n",
    "    z_scores = z_scores_joint_att + z_scores_self_att\n",
    "    for token,z_score in zip(tokens,z_scores):\n",
    "        if token not in qwen_result_analysis:\n",
    "            qwen_result_analysis[token] = [z_score]\n",
    "        else:\n",
    "            qwen_result_analysis[token].append(z_score)\n",
    "qwen_result_analysis = {k:(sum(v),len(v)) for k,v in qwen_result_analysis.items()}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "glm_result_analysis = {}\n",
    "for user in glm_result['OR2']['positive']:\n",
    "    join_att_after_seg = glm_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = glm_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    z_scores_joint_att = zscore (join_att)\n",
    "    z_scores_self_att = zscore (self_att)\n",
    "    z_scores = z_scores_joint_att + z_scores_self_att\n",
    "    for token,z_score in zip(tokens,z_scores):\n",
    "        if token not in glm_result_analysis:\n",
    "            glm_result_analysis[token] = [z_score]\n",
    "        else:\n",
    "            glm_result_analysis[token].append(z_score)\n",
    "glm_result_analysis = {k:(sum(v),len(v)) for k,v in glm_result_analysis.items()}\n",
    "\n",
    "glm_result_analysis_2 = {}\n",
    "for user in glm_result['OR2']['positive']:\n",
    "    join_att_after_seg = glm_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = glm_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    removed_joint_att_paris = glm_result['OR2']['positive'][user]['removed_joint_att_paris']\n",
    "    removed_self_att_paris = glm_result['OR2']['positive'][user]['removed_self_att_paris']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    removed_self_att = [pair[1] for pair in removed_self_att_paris]\n",
    "    removed_joint_att = [pair[1] for pair in removed_joint_att_paris]\n",
    "    mean_self_att = np.mean(self_att+removed_self_att)\n",
    "    mean_joint_att = np.mean(join_att+removed_joint_att)\n",
    "    std_dev_self_att = np.std(self_att+removed_self_att)\n",
    "    std_dev_joint_att = np.std(join_att+removed_joint_att) \n",
    "    z_scores_joint_att = [(att-mean_joint_att)/std_dev_joint_att for att in join_att]\n",
    "    z_scores_self_att = [(att-mean_self_att)/std_dev_self_att for att in self_att]\n",
    "    z_scores = z_scores_joint_att + z_scores_self_att\n",
    "    for token,z_score in zip(tokens,z_scores):\n",
    "        if token not in glm_result_analysis_2:\n",
    "            glm_result_analysis_2[token] = [z_score]\n",
    "        else:\n",
    "            glm_result_analysis_2[token].append(z_score)\n",
    "glm_result_analysis_2 = {k:(sum(v),len(v)) for k,v in glm_result_analysis_2.items()}\n",
    "\n",
    "glm_result_analysis_3 = {}\n",
    "all_tokens = {}\n",
    "for user in glm_result['OR2']['positive']:\n",
    "    join_att_after_seg = glm_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = glm_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    atts = [x + y for x, y in zip(join_att, self_att)]\n",
    "    for token,att in zip(tokens,atts):\n",
    "        if token not in all_tokens:\n",
    "            all_tokens[token] = [att]\n",
    "        else:\n",
    "            all_tokens[token].append(att)\n",
    "all_token_mean = np.mean([att for att_list in all_tokens.values() for att in att_list])\n",
    "all_token_std = np.std([att for att_list in all_tokens.values() for att in att_list])\n",
    "for user in glm_result['OR2']['positive']:\n",
    "    join_att_after_seg = glm_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = glm_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    atts = [x + y for x, y in zip(join_att, self_att)]\n",
    "    atts_zscore = [(att-all_token_mean)/all_token_std for  att in atts]\n",
    "    for token,z_score in zip(tokens,atts_zscore):\n",
    "        if token not in glm_result_analysis_3:\n",
    "            glm_result_analysis_3[token] = [z_score]\n",
    "        else:\n",
    "            glm_result_analysis_3[token].append(z_score)\n",
    "glm_result_analysis_3 = {k:sum(v) for k,v in glm_result_analysis_3.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_result_analysis_2 = {}\n",
    "for user in qwen_result['OR2']['positive']:\n",
    "    join_att_after_seg = qwen_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = qwen_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    removed_joint_att_paris = qwen_result['OR2']['positive'][user]['removed_joint_att_paris']\n",
    "    removed_self_att_paris = qwen_result['OR2']['positive'][user]['removed_self_att_paris']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    removed_self_att = [pair[1] for pair in removed_self_att_paris]\n",
    "    removed_joint_att = [pair[1] for pair in removed_joint_att_paris]\n",
    "    mean_self_att = np.mean(self_att+removed_self_att)\n",
    "    mean_joint_att = np.mean(join_att+removed_joint_att)\n",
    "    std_dev_self_att = np.std(self_att+removed_self_att)\n",
    "    std_dev_joint_att = np.std(join_att+removed_joint_att) \n",
    "    z_scores_joint_att = [(att-mean_joint_att)/std_dev_joint_att for att in join_att]\n",
    "    z_scores_self_att = [(att-mean_self_att)/std_dev_self_att for att in self_att]\n",
    "    #z_scores = z_scores_joint_att + z_scores_self_att\n",
    "    z_scores = [x + y for x, y in zip(z_scores_joint_att, z_scores_self_att)]\n",
    "    for token,z_score in zip(tokens,z_scores):\n",
    "        if token not in qwen_result_analysis_2:\n",
    "            qwen_result_analysis_2[token] = [z_score]\n",
    "        else:\n",
    "            qwen_result_analysis_2[token].append(z_score)\n",
    "qwen_result_analysis_2 = {k:(sum(v),len(v)) for k,v in qwen_result_analysis_2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_result_analysis_3 = {}\n",
    "all_tokens = {}\n",
    "for user in qwen_result['OR2']['positive']:\n",
    "    join_att_after_seg = qwen_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = qwen_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    atts = [x + y for x, y in zip(join_att, self_att)]\n",
    "    for token,att in zip(tokens,atts):\n",
    "        if token not in all_tokens:\n",
    "            all_tokens[token] = [att]\n",
    "        else:\n",
    "            all_tokens[token].append(att)\n",
    "all_token_mean = np.mean([att for att_list in all_tokens.values() for att in att_list])\n",
    "all_token_std = np.std([att for att_list in all_tokens.values() for att in att_list])\n",
    "for user in qwen_result['OR2']['positive']:\n",
    "    join_att_after_seg = qwen_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = qwen_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    atts = [x + y for x, y in zip(join_att, self_att)]\n",
    "    atts_zscore = [(att-all_token_mean)/all_token_std for  att in atts]\n",
    "    for token,z_score in zip(tokens,atts_zscore):\n",
    "        if token not in qwen_result_analysis_3:\n",
    "            qwen_result_analysis_3[token] = [z_score]\n",
    "        else:\n",
    "            qwen_result_analysis_3[token].append(z_score)\n",
    "qwen_result_analysis_3 = {k:sum(v) for k,v in qwen_result_analysis_3.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "llama_result_analysis = {}\n",
    "for user in llama_result['OR2']['positive']:\n",
    "    join_att_after_seg = llama_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = llama_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    z_scores_joint_att = zscore (join_att)\n",
    "    z_scores_self_att = zscore (self_att)\n",
    "    z_scores = z_scores_joint_att + z_scores_self_att\n",
    "    for token,z_score in zip(tokens,z_scores):\n",
    "        if token not in llama_result_analysis:\n",
    "            llama_result_analysis[token] = [z_score]\n",
    "        else:\n",
    "            llama_result_analysis[token].append(z_score)\n",
    "llama_result_analysis = {k:(sum(v),len(v)) for k,v in llama_result_analysis.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_result_analysis_2 = {}\n",
    "for user in llama_result['OR2']['positive']:\n",
    "    join_att_after_seg = llama_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = llama_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    removed_joint_att_paris = llama_result['OR2']['positive'][user]['removed_joint_att_paris']\n",
    "    removed_self_att_paris = llama_result['OR2']['positive'][user]['removed_self_att_paris']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    removed_self_att = [pair[1] for pair in removed_self_att_paris]\n",
    "    removed_joint_att = [pair[1] for pair in removed_joint_att_paris]\n",
    "    mean_self_att = np.mean(self_att+removed_self_att)\n",
    "    mean_joint_att = np.mean(join_att+removed_joint_att)\n",
    "    std_dev_self_att = np.std(self_att+removed_self_att)\n",
    "    std_dev_joint_att = np.std(join_att+removed_joint_att) \n",
    "    z_scores_joint_att = [(att-mean_joint_att)/std_dev_joint_att for att in join_att]\n",
    "    z_scores_self_att = [(att-mean_self_att)/std_dev_self_att for att in self_att]\n",
    "    z_scores = z_scores_joint_att + z_scores_self_att\n",
    "    for token,z_score in zip(tokens,z_scores):\n",
    "        if token not in llama_result_analysis_2:\n",
    "            llama_result_analysis_2[token] = [z_score]\n",
    "        else:\n",
    "            llama_result_analysis_2[token].append(z_score)\n",
    "llama_result_analysis_2 = {k:(sum(v),len(v)) for k,v in llama_result_analysis_2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_result_analysis_3 = {}\n",
    "all_tokens = {}\n",
    "for user in llama_result['OR2']['positive']:\n",
    "    join_att_after_seg = llama_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = llama_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    atts = [x + y for x, y in zip(join_att, self_att)]\n",
    "    for token,att in zip(tokens,atts):\n",
    "        if token not in all_tokens:\n",
    "            all_tokens[token] = [att]\n",
    "        else:\n",
    "            all_tokens[token].append(att)\n",
    "all_token_mean = np.mean([att for att_list in all_tokens.values() for att in att_list])\n",
    "all_token_std = np.std([att for att_list in all_tokens.values() for att in att_list])\n",
    "for user in llama_result['OR2']['positive']:\n",
    "    join_att_after_seg = llama_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = llama_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    atts = [x + y for x, y in zip(join_att, self_att)]\n",
    "    atts_zscore = [(att-all_token_mean)/all_token_std for  att in atts]\n",
    "    for token,z_score in zip(tokens,atts_zscore):\n",
    "        if token not in llama_result_analysis_3:\n",
    "            llama_result_analysis_3[token] = [z_score]\n",
    "        else:\n",
    "            llama_result_analysis_3[token].append(z_score)\n",
    "llama_result_analysis_3 = {k:sum(v) for k,v in llama_result_analysis_3.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "glm_result_analysis = {}\n",
    "for user in glm_result['OR2']['positive']:\n",
    "    join_att_after_seg = glm_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = glm_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    z_scores_joint_att = zscore (join_att)\n",
    "    z_scores_self_att = zscore (self_att)\n",
    "    z_scores = z_scores_joint_att + z_scores_self_att\n",
    "    for token,z_score in zip(tokens,z_scores):\n",
    "        if token not in glm_result_analysis:\n",
    "            glm_result_analysis[token] = [z_score]\n",
    "        else:\n",
    "            glm_result_analysis[token].append(z_score)\n",
    "glm_result_analysis = {k:(sum(v),len(v)) for k,v in glm_result_analysis.items()}\n",
    "\n",
    "glm_result_analysis_2 = {}\n",
    "for user in glm_result['OR2']['positive']:\n",
    "    join_att_after_seg = glm_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = glm_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    removed_joint_att_paris = glm_result['OR2']['positive'][user]['removed_joint_att_paris']\n",
    "    removed_self_att_paris = glm_result['OR2']['positive'][user]['removed_self_att_paris']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    removed_self_att = [pair[1] for pair in removed_self_att_paris]\n",
    "    removed_joint_att = [pair[1] for pair in removed_joint_att_paris]\n",
    "    mean_self_att = np.mean(self_att+removed_self_att)\n",
    "    mean_joint_att = np.mean(join_att+removed_joint_att)\n",
    "    std_dev_self_att = np.std(self_att+removed_self_att)\n",
    "    std_dev_joint_att = np.std(join_att+removed_joint_att) \n",
    "    z_scores_joint_att = [(att-mean_joint_att)/std_dev_joint_att for att in join_att]\n",
    "    z_scores_self_att = [(att-mean_self_att)/std_dev_self_att for att in self_att]\n",
    "    z_scores = z_scores_joint_att + z_scores_self_att\n",
    "    for token,z_score in zip(tokens,z_scores):\n",
    "        if token not in glm_result_analysis_2:\n",
    "            glm_result_analysis_2[token] = [z_score]\n",
    "        else:\n",
    "            glm_result_analysis_2[token].append(z_score)\n",
    "glm_result_analysis_2 = {k:(sum(v),len(v)) for k,v in glm_result_analysis_2.items()}\n",
    "\n",
    "glm_result_analysis_3 = {}\n",
    "all_tokens = {}\n",
    "for user in glm_result['OR2']['positive']:\n",
    "    join_att_after_seg = glm_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = glm_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    atts = [x + y for x, y in zip(join_att, self_att)]\n",
    "    for token,att in zip(tokens,atts):\n",
    "        if token not in all_tokens:\n",
    "            all_tokens[token] = [att]\n",
    "        else:\n",
    "            all_tokens[token].append(att)\n",
    "all_token_mean = np.mean([att for att_list in all_tokens.values() for att in att_list])\n",
    "all_token_std = np.std([att for att_list in all_tokens.values() for att in att_list])\n",
    "for user in glm_result['OR2']['positive']:\n",
    "    join_att_after_seg = glm_result['OR2']['positive'][user]['join_att_after_seg']\n",
    "    self_att_after_seg = glm_result['OR2']['positive'][user]['self_att_after_seg']\n",
    "    assert [pair[0] for pair in join_att_after_seg] == [pair[0] for pair in self_att_after_seg]\n",
    "    tokens = [pair[0] for pair in join_att_after_seg]\n",
    "    self_att = [pair[1] for pair in self_att_after_seg]\n",
    "    join_att = [pair[1] for pair in join_att_after_seg]\n",
    "    atts = [x + y for x, y in zip(join_att, self_att)]\n",
    "    atts_zscore = [(att-all_token_mean)/all_token_std for  att in atts]\n",
    "    for token,z_score in zip(tokens,atts_zscore):\n",
    "        if token not in glm_result_analysis_3:\n",
    "            glm_result_analysis_3[token] = [z_score]\n",
    "        else:\n",
    "            glm_result_analysis_3[token].append(z_score)\n",
    "glm_result_analysis_3 = {k:sum(v) for k,v in glm_result_analysis_3.items()}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
